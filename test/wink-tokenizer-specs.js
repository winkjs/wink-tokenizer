//     wink-tokenizer
//     Multilingual tokenizer that automatically tags each token with its type.
//
//     Copyright (C) 2017-18  GRAYPE Systems Private Limited
//
//     This file is part of ‚Äúwink-tokenizer‚Äù.
//
//     Permission is hereby granted, free of charge, to any person obtaining a
//     copy of this software and associated documentation files (the "Software"),
//     to deal in the Software without restriction, including without limitation
//     the rights to use, copy, modify, merge, publish, distribute, sublicense,
//     and/or sell copies of the Software, and to permit persons to whom the
//     Software is furnished to do so, subject to the following conditions:
//
//     The above copyright notice and this permission notice shall be included
//     in all copies or substantial portions of the Software.
//
//     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
//     OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
//     FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
//     THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
//     LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
//     FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
//     DEALINGS IN THE SOFTWARE.

//
var chai = require( 'chai' );
var mocha = require( 'mocha' );
var t = require( '../src/wink-tokenizer.js' );

var expect = chai.expect;
var describe = mocha.describe;
var it = mocha.it;

// NOTE: Sequence of test cases is important.
describe( 'wink tokenizer', function () {
  var tokenizer = t();
  var tokenize = tokenizer.tokenize;
  var fp = tokenizer.getTokensFP;

  it( 'should tokenize a very complex sentence', function () {
    var output = [ { value: '@superman', tag: 'mention' },
                   { value: ':', tag: 'punctuation' },
                   { value: 'hit', tag: 'word' },
                   { value: 'me', tag: 'word' },
                   { value: 'up', tag: 'word' },
                   { value: 'on', tag: 'word' },
                   { value: 'my', tag: 'word' },
                   { value: 'email', tag: 'word' },
                   { value: 'r2d2@gmail.com', tag: 'email' },
                   { value: ';', tag: 'punctuation' },
                   { value: '&', tag: 'symbol' },
                   { value: 'we', tag: 'word' },
                   { value: 'will', tag: 'word' },
                   { value: 'plan', tag: 'word' },
                   { value: 'party', tag: 'word' },
                   { value: 'üéâ', tag: 'emoji' },
                   { value: 'üéâ', tag: 'emoji' },
                   { value: 'üéâ', tag: 'emoji' },
                   { value: 'üéâ', tag: 'emoji' },
                   { value: '<3', tag: 'emoticon' },
                   { value: '4pm', tag: 'time' },
                   { value: ':D', tag: 'emoticon' },
                   { value: 'üéâ', tag: 'emoji' },
                   { value: 'tom', tag: 'word' },
                   { value: 'at', tag: 'word' },
                   { value: '3pm', tag: 'time' },
                   { value: ':)', tag: 'emoticon' },
                   { value: ':)', tag: 'emoticon' },
                   { value: '#fun', tag: 'hashtag' } ];
    expect( tokenize( '@superman: hit me up on my email r2d2@gmail.com; & we will plan partyüéâüéâüéâ üéâ  <34pm:D    üéâ tom at 3pm:):) #fun' ) ).to.deep.equal( output );
  } );

  it( 'should gnerate the finger print correctly', function () {
    expect( fp() ).to.equal( 'm:wwwwwwe;&wwwwjjjjctcjwwtcch' );
  } );

  it( 'should return an empty array with blank sentence', function () {
    expect( tokenize( '' ) ).to.deep.equal( [] );
    expect( tokenize( '  ' ) ).to.deep.equal( [] );
  } );

  it( 'should tokenize a simple sentence', function () {
    expect( tokenize( 'feeling good #FunTime' ) ).to.deep.equal( [ { value: 'feeling', tag: 'word' }, { value: 'good', tag: 'word' }, { value: '#FunTime', tag: 'hashtag' } ] );
  } );

  it( 'should gnerate the finger print correctly', function () {
    expect( fp() ).to.equal( 'wwh' );
  } );

  it( 'should tokenize a simple sentence with hashtag off', function () {
    expect( tokenizer.defineConfig( { hashtag: false } ) ).to.equal( 13 );
    expect( tokenize( 'feeling good #fun' ) ).to.deep.equal( [ { value: 'feeling', tag: 'word' }, { value: 'good', tag: 'word' }, { value: '#', tag: 'symbol' }, { value: 'fun', tag: 'word' } ] );
  } );

  it( 'should tokenize a complex sentence with full config', function () {
    var output = [ { value: '@superman', tag: 'mention' },
                   { value: ':', tag: 'punctuation' },
                   { value: 'hit', tag: 'word' },
                   { value: 'me', tag: 'word' },
                   { value: 'up', tag: 'word' },
                   { value: 'on', tag: 'word' },
                   { value: 'my', tag: 'word' },
                   { value: 'email', tag: 'word' },
                   { value: 'r2d2@gmail.com', tag: 'email' },
                   { value: ';', tag: 'punctuation' },
                   { value: '&', tag: 'symbol' },
                   { value: 'we', tag: 'word' },
                   { value: 'will', tag: 'word' },
                   { value: 'plan', tag: 'word' },
                   { value: 'party', tag: 'word' },
                   { value: 'üéâ', tag: 'emoji' },
                   { value: 'tom', tag: 'word' },
                   { value: 'at', tag: 'word' },
                   { value: '3pm', tag: 'time' },
                   { value: ':)', tag: 'emoticon' } ];
    expect( tokenize( '@superman: hit me up on my email r2d2@gmail.com;& we will plan partyüéâ tom at 3pm:)' ) ).to.deep.equal( output );
  } );

  it( 'should gnerate the finger print correctly for complex sentence', function () {
    expect( fp() ).to.equal( 'm:wwwwwwe;&wwwwjwwtc' );
  } );

  it( 'should tokenize a complex sentence when adding a custom regex', function () {
    var output = [ { value: '@superman', tag: 'mention' },
                   { value: ':', tag: 'punctuation' },
                   { value: 'come', tag: 'word' },
                   { value: 'help', tag: 'word' },
                   { value: 'me', tag: 'word' },
                   { value: 'out', tag: 'word' },
                   { value: 'I', tag: 'word' },
                   { value: '\'m', tag: 'word' },
                   { value: 'sick', tag: 'word' },
                   { value: '+o(', tag: 'emoticon' },
                   { value: ';', tag: 'punctuation' },
                   { value: '(oo)', tag: 'emoticon' },
                   { value: '<-', tag: 'emoticon' }
                  ];

    tokenizer.addRegex(/:\||O\.O|:`\(|\+o\(|\(oo\)|:%|:\$|>\|<|<-/gi, 'emoticon');
    expect( tokenizer.tokenize( '@superman: come help me out I\'m sick +o(; (oo) <-' ) ).to.deep.equal( output );
  } );

  it( 'should throw an error when adding a regex with an inexisting tag', function () {
    expect( function () {
      tokenizer.addRegex(/\(oo\)/gi, 'pig');
    } ).to.throw('Tag pig doesn\'t exist; Provide a \'fingerprintCode\' to add it as a tag.');
  } );

  it( 'should throw an error when adding a tag that already exists', function () {
    expect( function () {
      tokenizer.addTag('emoticon', 8);
    } ).to.throw('Tag emoticon already exists');
  } );

  it( 'should tokenize a complex sentence along with FP when adding a custom regex with a new tag', function () {
    // Expected output with custom regex.
    var output = [ { value: 'I', tag: 'word' },
                   { value: '\'m', tag: 'word' },
                   { value: 'thinking', tag: 'word' },
                   { value: 'why', tag: 'word' },
                   { value: 'superman', tag: 'superman' },
                   { value: '\'', tag: 'punctuation' },
                   { value: 's', tag: 'word' },
                   { value: 'dead', tag: 'word' },
                   { value: '?', tag: 'punctuation' },
                   { value: '!', tag: 'punctuation' }
                  ];

    // Expected output after a defineConfig() call.
    var output1 = [ { value: 'I', tag: 'word' },
                   { value: '\'m', tag: 'word' },
                   { value: 'thinking', tag: 'word' },
                   { value: 'why', tag: 'word' },
                   { value: 'superman', tag: 'word' },
                   { value: '\'s', tag: 'word' },
                   { value: 'dead', tag: 'word' },
                   { value: '?', tag: 'punctuation' },
                   { value: '!', tag: 'punctuation' }
                  ];
    // Add custome regex/tag/fp.
    tokenizer.addRegex(/superman/gi, 'superman', 's');
    // Test tokenization.
    expect( tokenizer.tokenize( 'I\'m thinking why superman\'s dead ?!' ) ).to.deep.equal( output );
    // Test finger printing.
    expect( tokenizer.getTokensFP() ).to.deep.equal( 'wwwws\'ww?!' );
    // Call defineConfig to reset the above.
    tokenizer.defineConfig( { word: true } );
    // Repeat the above tests to confirm the effect of reset.
    expect( tokenizer.tokenize( 'I\'m thinking why superman\'s dead ?!' ) ).to.deep.equal( output1 );
    expect( tokenizer.getTokensFP() ).to.deep.equal( 'wwwwwww?!' );
  } );

  it( 'should tokenize a complex sentence with empty config', function () {
    var output = [ { value: '@superman:', tag: 'alien' },
                   { value: 'hit', tag: 'alien' },
                   { value: 'me', tag: 'alien' },
                   { value: 'up', tag: 'alien' },
                   { value: 'on', tag: 'alien' },
                   { value: 'my', tag: 'alien' },
                   { value: 'email', tag: 'alien' },
                   { value: 'r2d2@gmail.com;', tag: 'alien' },
                   { value: '&', tag: 'alien' },
                   { value: 'we', tag: 'alien' },
                   { value: 'will', tag: 'alien' },
                   { value: 'plan', tag: 'alien' },
                   { value: 'partyüéâ', tag: 'alien' },
                   { value: 'tom', tag: 'alien' },
                   { value: 'at', tag: 'alien' },
                   { value: '3pm:)', tag: 'alien' } ];
    expect( tokenizer.defineConfig( {} ) ).to.equal( 0 );
    expect( tokenize( '@superman: hit me up on my email r2d2@gmail.com;  & we will plan partyüéâ tom at 3pm:)' ) ).to.deep.equal( output );
  } );

  it( 'should tokenize a complex sentence with possessive & aposrtophy stuff', function () {
    var output = [ { value: 'She', tag: 'word' },
                   { value: 'was', tag: 'word' },
                   { value: 'n\'t', tag: 'word' },
                   { value: 'at', tag: 'word' },
                   { value: 'home', tag: 'word' },
                   { value: 'and', tag: 'word' },
                   { value: 'wild', tag: 'word' },
                   { value: 'cats', tag: 'word' },
                   { value: '\'', tag: 'word' },
                   { value: 'ate', tag: 'word' },
                   { value: 'her', tag: 'word' },
                   { value: 'dog', tag: 'word' },
                   { value: '\'s', tag: 'word' },
                   { value: 'food', tag: 'word' } ];
    expect( t().tokenize( 'She wasn\'t at home and wild cats\' ate her dog\'s food' ) ).to.deep.equal( output );
  } );

  it( 'should tokenize a sentence with multiple contractions & containing extra spaces', function () {
    var output = [ { value: 'I', tag: 'word' },
                   { value: '\'ll', tag: 'word' },
                   { value: 'eat', tag: 'word' },
                   { value: 'John', tag: 'word' },
                   { value: '\'s', tag: 'word' },
                   { value: 'food', tag: 'word' },
                   { value: 'today', tag: 'word' },
                   { value: 'with', tag: 'word' },
                   { value: 'O\'kelly', tag: 'word' } ];
    expect( t().tokenize( '     I\'ll eat      John\'s food today with O\'kelly  ' ) ).to.deep.equal( output );
  } );

  it( 'should tokenize a sentence with words with diacritical marks', function () {
    var output = [ { value: 'Zo√´', tag: 'word' },
                   { value: 'submitted', tag: 'word' },
                   { value: 'her', tag: 'word' },
                   { value: 'r√©sum√©', tag: 'word' },
                   { value: 'üéâ', tag: 'emoji' },
                   { value: 'in', tag: 'word' },
                   { value: 'Nestl√©', tag: 'word' },
                   { value: ':-)', tag: 'emoticon' } ];
    expect( t().tokenize( 'Zo√´ submitted her r√©sum√©üéâ in Nestl√©:-)' ) ).to.deep.equal( output );
  } );

  it( 'should tokenize a sentence in french', function () {
    var output = [ { value: 'Petit', tag: 'word' },
                   { value: 'a', tag: 'word' },
                   { value: 'petit', tag: 'word' },
                   { value: ',', tag: 'punctuation' },
                   { value: 'l', tag: 'word' },
                   { value: '‚Äô', tag: 'punctuation' },
                   { value: 'oiseau', tag: 'word' },
                   { value: 'fait', tag: 'word' },
                   { value: 'son', tag: 'word' },
                   { value: 'nid', tag: 'word' } ];
    expect( t().tokenize( 'Petit a petit, l‚Äôoiseau fait son nid' ) ).to.deep.equal( output );
  } );

  it( 'should tokenize another sentence in french', function () {
    var output = [ { value: 'Mieux', tag: 'word' },
                   { value: 'vaut', tag: 'word' },
                   { value: 'pr√©venir', tag: 'word' },
                   { value: 'que', tag: 'word' },
                   { value: 'gu√©rir', tag: 'word' },
                   { value: ':)', tag: 'emoticon' } ];
    expect( t().tokenize( 'Mieux vaut pr√©venir que gu√©rir:)' ) ).to.deep.equal( output );
  } );

  it( 'should tokenize a sentence in german', function () {
    var output = [ { value: '√úbung', tag: 'word' },
                   { value: 'macht', tag: 'word' },
                   { value: 'den', tag: 'word' },
                   { value: 'Meister', tag: 'word' },
                   { value: '.', tag: 'punctuation' } ];
    expect( t().tokenize( '√úbung macht den Meister.' ) ).to.deep.equal( output );
  } );

  it( 'should tokenize a sentence in spanish', function () {
    var output = [ { value: 'Donde', tag: 'word' },
                   { value: 'hay', tag: 'word' },
                   { value: 'gana', tag: 'word' },
                   { value: ',', tag: 'punctuation' },
                   { value: 'hay', tag: 'word' },
                   { value: 'ma√±a', tag: 'word' },
                   { value: '.', tag: 'punctuation' } ];
    expect( t().tokenize( 'Donde hay gana, hay ma√±a.' ) ).to.deep.equal( output );
  } );

  it( 'should tokenize a sentence in icelandic', function () {
    var output = [ { value: 'Vinr', tag: 'word' },
                   { value: 'er', tag: 'word' },
                   { value: 's√°s', tag: 'word' },
                   { value: 'v√∂rnu√∞', tag: 'word' },
                   { value: 'b√Ω√∞r', tag: 'word' },
                   { value: '.', tag: 'punctuation' } ];
    expect( t().tokenize( 'Vinr er s√°s v√∂rnu√∞ b√Ω√∞r.' ) ).to.deep.equal( output );
  } );

  it( 'should tokenize a sentence containing lots of currency symbols', function () {
    var output = [ { value: 'I', tag: 'word' },
                   { value: 'have', tag: 'word' },
                   { value: '$', tag: 'currency' },
                   { value: '200.0', tag: 'number' },
                   { value: '‚Çø', tag: 'currency' },
                   { value: '2.0', tag: 'number' },
                   { value: 'is', tag: 'word' },
                   { value: '1', tag: 'number' },
                   { value: '%', tag: 'symbol' },
                   { value: ';', tag: 'punctuation' },
                   { value: '‚ÇΩ', tag: 'currency' },
                   { value: '100', tag: 'number' },
                   { value: '‚Çπ', tag: 'currency' },
                   { value: '200', tag: 'number' },
                   { value: '‚Ç®', tag: 'currency' },
                   { value: '300', tag: 'number' },
                   { value: '>', tag: 'symbol' },
                   { value: '>', tag: 'symbol' },
                   { value: '$', tag: 'currency' },
                   { value: '10000.00', tag: 'number' },
                   { value: ';', tag: 'punctuation' },
                   { value: '&', tag: 'symbol' },
                   { value: '¬£', tag: 'currency' },
                   { value: '2', tag: 'number' },
                   { value: '¬•', tag: 'currency' },
                   { value: '0.5', tag: 'number' },
                   { value: '‚Ç¨', tag: 'currency' },
                   { value: '1.2', tag: 'number' },
                   { value: '‚Ç©', tag: 'currency' },
                   { value: '1', tag: 'number' },
                   { value: ':-)', tag: 'emoticon' } ];
    expect( t().tokenize( 'I have$200.0 ‚Çø2.0 is 1%; ‚ÇΩ100‚Çπ200‚Ç®300 >> $10000.00; & ¬£2 ¬•0.5 ‚Ç¨1.2‚Ç©1:-)' ) ).to.deep.equal( output );
  } );
  it( 'should tokenize multi-script complex sentence', function () {
    var output = [ { value: '‡§¶‡§ø‡§ó‡•ç‡§ó‡§ú', tag: 'word' },
                   { value: '‡§∂‡§æ‡§Ø‡§∞', tag: 'word' },
                   { value: '‡§Æ‡§ø‡§∞‡•ç‡§ú‡§º‡§æ', tag: 'word' },
                   { value: '#Ghalib', tag: 'hashtag' },
                   { value: '‡§ï‡•Ä', tag: 'word' },
                   { value: '‡§™‡•Å‡§£‡•ç‡§Ø‡§§‡§ø‡§•‡§ø', tag: 'word' },
                   { value: '(', tag: 'punctuation' },
                   { value: '27', tag: 'number' },
                   { value: 'December', tag: 'word' },
                   { value: '‡•ß‡•≠‡•Ø‡•≠', tag: 'number' },
                   { value: ')', tag: 'punctuation' },
                   { value: '‡§™‡§∞', tag: 'word' },
                   { value: '‡§â‡§®‡§ï‡§æ', tag: 'word' },
                   { value: '‡§ï‡§ø‡§∞‡§¶‡§æ‡§∞', tag: 'word' },
                   { value: '‡§Ö‡§¶‡§æ', tag: 'word' },
                   { value: '‡§ï‡§∞‡§®‡•á', tag: 'word' },
                   { value: '‡§µ‡§æ‡§≤‡•á', tag: 'word' },
                   { value: '@NaseerudinShah', tag: 'mention' },
                   { value: '‡§®‡•á', tag: 'word' },
                   { value: '‡§ï‡•ç‡§Ø‡§æ', tag: 'word' },
                   { value: '‡§ï‡§π‡§æ', tag: 'word' },
                   { value: ',', tag: 'punctuation' },
                   { value: '‡§Ü‡§™', tag: 'word' },
                   { value: '‡§≠‡•Ä', tag: 'word' },
                   { value: '‡§∏‡•Å‡§®‡§ø‡§è', tag: 'word' },
                   { value: 'üëÇ', tag: 'emoji' },
                   { value: '‡•§', tag: 'punctuation' },
                   { value: ':p', tag: 'emoticon' } ];
    expect( t().tokenize( '‡§¶‡§ø‡§ó‡•ç‡§ó‡§ú ‡§∂‡§æ‡§Ø‡§∞ ‡§Æ‡§ø‡§∞‡•ç‡§ú‡§º‡§æ #Ghalib ‡§ï‡•Ä ‡§™‡•Å‡§£‡•ç‡§Ø‡§§‡§ø‡§•‡§ø (27 December ‡•ß‡•≠‡•Ø‡•≠) ‡§™‡§∞ ‡§â‡§®‡§ï‡§æ ‡§ï‡§ø‡§∞‡§¶‡§æ‡§∞ ‡§Ö‡§¶‡§æ ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•á @NaseerudinShah ‡§®‡•á ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§π‡§æ, ‡§Ü‡§™ ‡§≠‡•Ä ‡§∏‡•Å‡§®‡§ø‡§èüëÇ‡•§:p' ) ).to.deep.equal( output );
  } );

  it( 'should tokenize sanskrit gayatri mantra', function () {
    var output = [ { value: '‡•ê', tag: 'symbol' },
                   { value: '‡§≠‡•Ç‡§∞‡•ç‡§≠‡•Å‡§µ‡§É', tag: 'word' },
                   { value: '‡§∏‡•ç‡§µ‡§É', tag: 'word' },
                   { value: '‡•§', tag: 'punctuation' },
                   { value: '‡§§‡§§‡•ç‡§∏‡•ë‡§µ‡§ø‡•í‡§§‡•Å‡§∞‡•ç‡§µ‡§∞‡•á‡§£‡•ç‡§Ø‡§Ç‡•í', tag: 'word' },
                   { value: '‡§≠‡§∞‡•ç‡§ó‡•ã‡•ë', tag: 'word' },
                   { value: '‡§¶‡•á‡•í‡§µ‡§∏‡•ç‡§Ø‡•ë‡§ß‡•Ä‡§Æ‡§π‡§ø', tag: 'word' },
                   { value: '‡•§', tag: 'punctuation' },
                   { value: '‡§ß‡§ø‡§Ø‡•ã‡•í', tag: 'word' },
                   { value: '‡§Ø‡•ã', tag: 'word' },
                   { value: '‡§®‡§É‡•ë', tag: 'word' },
                   { value: '‡§™‡•ç‡§∞‡§ö‡•ã‡•í‡§¶‡§Ø‡§æ‡•ë‡§§‡•ç', tag: 'word' },
                   { value: '‡••', tag: 'punctuation' } ];
    expect( t().tokenize( '‡•ê ‡§≠‡•Ç‡§∞‡•ç‡§≠‡•Å‡§µ‡§É ‡§∏‡•ç‡§µ‡§É ‡•§ ‡§§‡§§‡•ç‡§∏‡•ë‡§µ‡§ø‡•í‡§§‡•Å‡§∞‡•ç‡§µ‡§∞‡•á‡§£‡•ç‡§Ø‡§Ç‡•í ‡§≠‡§∞‡•ç‡§ó‡•ã‡•ë ‡§¶‡•á‡•í‡§µ‡§∏‡•ç‡§Ø‡•ë‡§ß‡•Ä‡§Æ‡§π‡§ø ‡•§ ‡§ß‡§ø‡§Ø‡•ã‡•í ‡§Ø‡•ã ‡§®‡§É‡•ë ‡§™‡•ç‡§∞‡§ö‡•ã‡•í‡§¶‡§Ø‡§æ‡•ë‡§§‡•ç ‡••' ) ).to.deep.equal( output );
  } );

  it( 'should tokenize a marathi tweet', function () {
    var output = [ { value: '‡§Ü‡§ú‡§ö‡•á', tag: 'word' },
                   { value: '#‡§ü‡•ç‡§µ‡§ø‡§ü‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ‡§®', tag: 'hashtag' },
                   { value: '#‡§ü‡•ç‡§µ‡§ø‡§ü‡§∞‡§∏‡§Ç‡§Æ‡•á‡§≤‡§®', tag: 'hashtag' },
                   { value: '‡§µ‡§ø‡§∑‡§Ø', tag: 'word' },
                   { value: ':', tag: 'punctuation' },
                   { value: '"‡§õ‡§Ç‡§¶: ‡§è‡§ï ‡§Ü‡§Ø‡•Å‡§∑‡•ç‡§Ø‡§æ‡§ö‡•Ä ‡§∂‡•à‡§≤‡•Ä"', tag: 'quoted_phrase' },
                   { value: '‡§µ‡•á‡§≥', tag: 'word' },
                   { value: ':', tag: 'punctuation' },
                   { value: '‡§∞‡§æ‡§§‡•ç‡§∞‡•Ä', tag: 'word' },
                   { value: '.', tag: 'punctuation' },
                   { value: '‡•Ø.‡•¶‡•¶', tag: 'number' },
                   { value: '‡§§‡•á', tag: 'word' },
                   { value: '‡•ß‡•¶.‡•¶‡•¶', tag: 'number' },
                   { value: '‡§µ‡§ï‡•ç‡§§‡•á', tag: 'word' },
                   { value: ':', tag: 'punctuation' },
                   { value: '@hifrom_vinit', tag: 'mention' } ];
    expect( t().tokenize( '‡§Ü‡§ú‡§ö‡•á #‡§ü‡•ç‡§µ‡§ø‡§ü‡§µ‡•ç‡§Ø‡§æ‡§ñ‡•ç‡§Ø‡§æ‡§® #‡§ü‡•ç‡§µ‡§ø‡§ü‡§∞‡§∏‡§Ç‡§Æ‡•á‡§≤‡§® ‡§µ‡§ø‡§∑‡§Ø: "‡§õ‡§Ç‡§¶: ‡§è‡§ï ‡§Ü‡§Ø‡•Å‡§∑‡•ç‡§Ø‡§æ‡§ö‡•Ä ‡§∂‡•à‡§≤‡•Ä" ‡§µ‡•á‡§≥: ‡§∞‡§æ‡§§‡•ç‡§∞‡•Ä. ‡•Ø.‡•¶‡•¶ ‡§§‡•á ‡•ß‡•¶.‡•¶‡•¶ ‡§µ‡§ï‡•ç‡§§‡•á: @hifrom_vinit' ) ).to.deep.equal( output );
  } );

  it( 'should tokenize different number formats & ordinals', function () {
    var output = [ { value: '‡•Ø.‡•¶‡•¶', tag: 'number' },
                   { value: '‡§§‡•á', tag: 'word' },
                   { value: '‡•ß‡•¶.‡•¶‡•¶', tag: 'number' },
                   { value: '‡•©,‡•ß‡•®.‡•™‡•´‡•¨-‡•≠', tag: 'number' },
                   { value: 'funny', tag: 'word' },
                   { value: 'format', tag: 'word' },
                   { value: '!', tag: 'punctuation' },
                   { value: '2nd', tag: 'ordinal' },
                   { value: '33', tag: 'number' },
                   { value: 'th', tag: 'word' },
                   { value: '1', tag: 'number' },
                   { value: '2nd', tag: 'ordinal' },
                   { value: '11th', tag: 'ordinal' },
                   { value: '93rd', tag: 'ordinal' },
                   { value: '.', tag: 'punctuation' },
                   { value: 'my', tag: 'word' },
                   { value: '1st', tag: 'ordinal' },
                   { value: 'ever', tag: 'word' },
                   { value: 'ip', tag: 'word' },
                   { value: 'is', tag: 'word' },
                   { value: '8,8.8-8', tag: 'number' } ];
    expect( t().tokenize( '‡•Ø.‡•¶‡•¶ ‡§§‡•á ‡•ß‡•¶.‡•¶‡•¶ ‡•©,‡•ß‡•®.‡•™‡•´‡•¨-‡•≠funny format! 2nd 33th 12nd 11th 93rd. my 1st ever ip is 8,8.8-8' ) ).to.deep.equal( output );
  } );

  it( 'should tokenize a contractions (pronoun, verb & name) heavy sentence', function () {
    var output = [ { value: 'We', tag: 'word' },
                   { value: '\'ll', tag: 'word' },
                   { value: 'help', tag: 'word' },
                   { value: 'you', tag: 'word' },
                   { value: 'if', tag: 'word' },
                   { value: 'you', tag: 'word' },
                   { value: 'wo', tag: 'word' },
                   { value: 'n\'t', tag: 'word' },
                   { value: 'create', tag: 'word' },
                   { value: 'trouble', tag: 'word' },
                   { value: ',', tag: 'punctuation' },
                   { value: 'Jamie', tag: 'word' },
                   { value: 'O\'Hara', tag: 'word' } ];
    expect( t().tokenize( 'We\'ll help you if you won\'t create trouble, Jamie O\'Hara' ) ).to.deep.equal( output );
  } );
} );
