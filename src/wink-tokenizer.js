//     wink-tokenizer
//     Versatile tokenizer that automatically tags each token with its type.
//
//     Copyright (C) 2017  GRAYPE Systems Private Limited
//
//     This file is part of ‚Äúwink-sentiment‚Äù.
//
//     ‚Äúwink-sentiment‚Äù is free software: you can redistribute
//     it and/or modify it under the terms of the GNU Affero
//     General Public License as published by the Free
//     Software Foundation, version 3 of the License.
//
//     ‚Äúwink-sentiment‚Äù is distributed in the hope that it will
//     be useful, but WITHOUT ANY WARRANTY; without even
//     the implied warranty of MERCHANTABILITY or FITNESS
//     FOR A PARTICULAR PURPOSE.  See the GNU Affero General
//     Public License for more details.
//
//     You should have received a copy of the GNU Affero
//     General Public License along with ‚Äúwink-sentiment‚Äù.
//     If not, see <http://www.gnu.org/licenses/>.

//
var rgxSpaces = /\s+/g;
var rgxNumber = /\d*\.\d+|\d+/g;
var rgxTwitter = /\@\w+/g;
var rgxEmail = /[-!#$%&'*+\/=?^\w{|}~](?:\.?[-!#$%&'*+\/=?^\w`{|}~])*@[a-z0-9](?:-?\.?[a-z0-9])*(?:\.[a-z](?:-?[a-z0-9])*)+/gi;
var rgxCurrency = /[\$\¬£\¬•\‚Ç¨]/g;
var rgxPunctuation = /[\‚Äô\'\‚Äò\‚Äô\`\‚Äú\‚Äù\"\[\]\(\)\{\}\‚Ä¶\,\.\!\;\?\/\-\:]/g;
var rgxQuotedPhrase = /\"[^\"]*\"/g;
var rgxURL = /(?:https?:\/\/)(?:[\da-z\.-]+)\.(?:[a-z\.]{2,6})(?:[\/\w\.\-\?#=]*)*\/?/gi;
var rgxEmoji = /[\uD800-\uDBFF][\uDC00-\uDFFF]|[\u2600-\u26FF]|[\u2700-\u27BF]/g;
var rgxEmoticon = /:-?[dps\*\/\[\]\{\}\(\)]|;-?[/(/)d]|<3/gi;
var rgxTime = /(?:\d|[01]\d|2[0-3]):?(?:[0-5][0-9])?\s?(?:[ap]m|hours|hrs)\b/gi;
// Regexes and their categories; used for tokenizing via match/split. The
// sequence is *critical* for correct tokenization.
var rgxs = [
  { regex: rgxQuotedPhrase, category: 'quoted_phrase' },
  { regex: rgxURL, category: 'url' },
  { regex: rgxEmail, category: 'email' },
  { regex: rgxTwitter, category: 'twitter' },
  { regex: rgxEmoji, category: 'emoji' },
  { regex: rgxEmoticon, category: 'emoticon' },
  { regex: rgxTime, category: 'time' },
  { regex: rgxNumber, category: 'number' },
  { regex: rgxCurrency, category: 'currency' },
  { regex: rgxPunctuation, category: 'punctuation' }
];
// Used to generate finger print from the tokens.
var fingerPrintCodes = {
  twitter: 'b',
  emoticon: 'c',
  email: 'e',
  emoji: 'j',
  weekday: 'k',
  month: 'm',
  number: 'n',
  quoted_phrase: 'q', // eslint-disable-line camelcase
  currency: 'r',
  time: 't',
  url: 'u',
  word: 'w',
};
// Simple in-tokeizer lookup.
var weekday = 'weekday';
var month = 'month';
var lookup = Object.create( null );
// Setu weekdays.
lookup.mon = weekday;
lookup.monday = weekday;
lookup.tue = weekday;
lookup.tuesday = weekday;
lookup.wed = weekday;
lookup.wednesday = weekday;
lookup.thu = weekday;
lookup.thursday = weekday;
lookup.fri = weekday;
lookup.friday = weekday;
lookup.sat = weekday;
lookup.saturday = weekday;
lookup.sun = weekday;
lookup.sunday = weekday;
// Setup months;
lookup.jan = month;
lookup.january = month;
lookup.feb = month;
lookup.february = month;
lookup.mar = month;
lookup.march = month;
lookup.apr = month;
lookup.april = month;
lookup.may = month;
lookup.jun = month;
lookup.june = month;
lookup.jul = month;
lookup.july = month;
lookup.aug = month;
lookup.august = month;
lookup.sep = month;
lookup.september = month;
lookup.oct = month;
lookup.october = month;
lookup.nov = month;
lookup.november = month;
lookup.dec = month;
lookup.december = month;

// ### tokenizer
/**
 *
 * Creates an instance of **`wink-tokenizer`**.
 *
 * @return {methods} object conatining set of API methods for tokenizing a sentence
 * and defining configuration, plugin etc.
 * @example
 * // Load wink tokenizer.
 * var tokenizer = require( 'wink-tokenizer' );
 * // Create your instance of wink tokenizer.
 * var myTokenizer = tokenizer();
*/
var tokenizer = function () {

  var finalTokens = [];
  var tagPlugins = Object.create( null );
  var methods = Object.create( null );

  // ### tokenizeTextUnit
  /**
   *
   * Attempts to tokenize the input `text` using the `rgxSplit`. The tokenization
   * is carried out by combining the regex matches and splits in the right sequence.
   * The matches are the *real tokens*, whereas splits are text units that are
   * tokenized in later rounds! The real tokens (i.e. matches) are pushed as
   * `object` and splits as `string`.
   *
   * @param {string} text ‚Äî unit that is to be tokenized.
   * @param {object} rgxSplit ‚Äî object containing the regex and it's category.
   * @return {array} of tokens.
   * @private
  */
  var tokenizeTextUnit = function ( text, rgxSplit ) {
    // Regex matches go here; note each match is a token and has the same tag
    // as of regex's category.
    var matches = text.match( rgxSplit.regex );
    // Balance is "what needs to be tokenized".
    var balance = text.split( rgxSplit.regex );
    // The result, in form of combination of tokens & matches, is captured here.
    var tokens = [];
    // Helper variables.
    var i,
        imax,
        k = 0,
        t;

    // Combine tokens & matches in the following pattern [ b0 m0 b1 m1 ... ]
    matches = ( matches ) ? matches : [];
    for ( i = 0, imax = balance.length; i < imax; i += 1 ) {
      t = balance[ i ];
      t = t.trim();
      if ( t ) tokens.push( t );
      if ( k < matches.length ) tokens.push( { token: matches[ k ], tag: rgxSplit.category } );
      k += 1;
    }

    return ( tokens );
  }; // tokenizeTextUnit()

  // ### tokenizeTextRecursively
  /**
   *
   * Tokenizes the input text recursively using the array of `regexes` and then
   * the `tokenizeTextUnit()` function. If (or whenever) the `regexes` becomes
   * empty, it simply splits the text on non-word characters instead of using
   * the `tokenizeTextUnit()` function.
   *
   * @param {string} text ‚Äî unit that is to be tokenized.
   * @param {object} regexes ‚Äî object containing the regex and it's category.
   * @return {undefined} nothing!
   * @private
  */
  var tokenizeTextRecursively = function ( text, regexes ) {
    var sentence = text.trim();
    var tokens = [];
    var i, imax;
    if ( !regexes.length ) {
      // Empty `regexes` array, split on non-word characters.
      if ( sentence ) {
        sentence.split( /\W/ ).forEach( function ( t ) {
          var tag;
          if ( t ) {
            tag = lookup[ t.toLowerCase() ];
            finalTokens.push( { token: t, tag: ( tag ) ? tag : 'word' } );
          }
        } );
      }
      return;
    }

    var rgx = regexes[ 0 ];
    tokens = tokenizeTextUnit( sentence, rgx );

    for ( i = 0, imax = tokens.length; i < imax; i += 1 ) {
      if ( typeof tokens[ i ] === 'string' ) {
        // Strings become candidates for further tokenization.
        tokenizeTextRecursively( tokens[ i ], regexes.slice( 1 ) );
      } else {
        finalTokens.push( tokens[ i ] );
      }
    }
  }; // tokenizeTextRecursively()

  // ### defineConfig
  /**
   *
   * Defines the configuration in terms of the types of token that will be
   * extracted by [`tokenize()`](#tokenize) method. Note by default, all types
   * of tokens will be detected and tagged automatically.
   *
   * @param {object} config ‚Äî It defines 0 or more properties from the list of
   * **10** properties given below. A true value for a property ensures tokenization
   * for that type of text wheras false value will not attempt tokenization of that
   * type of text. *An empty config object means only **words** are extracted.* A
   * word can be composed of only **alphabets** and **`_`** character.
   * @param {boolean} [config.currency=true] such as **$** or **¬£** symbols
   * @param {boolean} [config.email=true] for example **john@acme.com** or **superman1@gmail.com**
   * @param {boolean} [config.emoji=true] any standard unicode emojis e.g. üòä or üòÇ or üéâ
   * @param {boolean} [config.emoticon=true] common emoticons such as **`:-)`** or **`:D`**
   * @param {boolean} [config.month=true] months of the year and their standard
   * abbreviations such as **March** or **Mar** or **jan**
   * @param {boolean} [config.number=true] any integer or decimal number such as **19** or **2.718**
   * @param {boolean} [config.punctuation=true] common punctuation such as **`?`** or **`,`**
   * @param {boolean} [config.quoted_phrase=true] any **"quoted text"** in the sentence.
   * @param {boolean} [config.time=true] common representation of time such as **4pm** or **16:00 hours**
   * @param {boolean} [config.twitter=true] twitter **@handle**
   * @param {boolean} [config.url=true] URL such as **https://github.com**
   * @return {number} number of properties set to true from the list of above 10.
   * @example
   * // Do not tokenize & tag twitter handles.
   * var myTokenizer.defineConfig( { twitter: false } );
   * // -> 9
   * // Only tokenize words as defined above.
   * var myTokenizer.defineConfig( {} );
   * // -> 0
  */
  var defineConfig = function ( config ) {
    if ( typeof config === 'object' && Object.keys( config ).length ) {
      rgxs = rgxs.filter( function ( rgx ) {
        // Config for the Category of `rgx`.
        var cc = config[ rgx.category ];
        // Means `undefined` & `null` values are taken as true; otherwise
        // standard **truthy** and **falsy** interpretation applies!!
        return ( cc === undefined || cc === null || !!cc );
      } );
    } else rgxs = [];
    return rgxs.length;
  }; // defineConfig()

  var definePlugin = function ( tag, functions ) {
    if ( tagPlugins[ tag ] === undefined ) {
      throw Error( ':(' );
    }
    tagPlugins[ tag ] = functions;
  }; // addPlugin

  var tokenize = function ( sentence ) {
    finalTokens = [];
    tokenizeTextRecursively( sentence.trim().replace( rgxSpaces, ' ' ), rgxs );
    return finalTokens;
  }; // tokenize()

  var getFingerprint = function () {
    var fp = [];
    finalTokens.forEach( function ( t ) {
      fp.push( ( fingerPrintCodes[ t.tag ] ) ? fingerPrintCodes[ t.tag ] : t.token );
    } );
    return fp.join( '' );
  }; // getFingerprint()

  rgxs.forEach( function ( rgx ) {
    tagPlugins[ rgx.category ] = false;
  } );
  methods.defineConfig = defineConfig;
  methods.definePlugin = definePlugin;
  methods.tokenize = tokenize;
  methods.getFingerprint = getFingerprint;
  return methods;
};

module.exports = tokenizer;

// var t = tokenizer();
//
// var pt = t.tokenize;
//
// console.log( t.defineConfig( {  } ) );
//
// console.time( 'perf' );
// console.log( pt( '1abc@pappu.org<3 I,    üòÇüòÇ  23.456713   sanjayaüòÇ saxena:), sent <3 an $2.5 emails meet at 5:00pm:) to @prtksxna on his id -prtk.Sxna@gmail.com.') );
// console.timeEnd( 'perf' );
// console.time( 'perf' );
// console.log( pt( 'Fix the following 2 issues "fix me", "fix yourself" on http://github.com/winkjs/wink-sentiment/issues:) and then party üéâ' ) );
// console.timeEnd( 'perf' );
// console.time( 'perf' );
// console.log( pt( 'Fix the following 1st January 1992 issue "fix yourself" on 2-mon-ey-13 http://github.com/winkjs/wink-sentiment/issues:) and then party üéâ' ) );
// console.timeEnd( 'perf' );
// console.log( t.getFingerprint() );
//
// // pt( 'I ate 3 mangos and 4.2bananas3');
// // pt( 'I ate bananas2.')
// // pt( '' );
